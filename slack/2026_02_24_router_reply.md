Sure, I'll try a hard-only LCB subset (probably will need v4, to make the size large enough). Current 100-problem set is mixed difficulty which limits headroom similar to what happened with the full-scale math runs.

On consistency: not really. I compared two identical baseline runs (63% and 58%, same config, same 100 problems) and 35 problems changed outcome between them just from sampling variance. Against that backdrop, concepts helped 15 and hurt 13 problems. Of those, only 6 helped and 6 hurt are "stable" (consistent across both baselines). The other 9 helped and 7 hurt overlap with the baseline variance: can't tell if concepts actually did anything or if the model just got lucky/unlucky. So the honest answer is about 6 problems are consistently helped out of 100.

On metacog reuse / RLAD: yeah, I will do this next, since code is absent for both, I am assuming just borrow/test their high level ideas?

On the router: yes, the current router is an extra filter on selected concepts: it (LLM/NLI) takes the retrieved/selected concepts and asks which are relevant, drops the rest. I did think about binary gating too but ruled it out because I was confident models could retain good concepts while removing all concepts could hurt. Thinking again though, if the goal is preserving solve rate on simple problems, binary gating should be tested, and it fits the definition of a router better (what I implemented is more like a cleanup process). For defaulting to no concepts and only retrying with concepts after a failed first attempt: self-reflection works for ARC but I'm not sure it applies cleanly to the other benchmarks. Claude suggested using external reflection directly but I vetoed it as a cheating approach. I think self-reflection can be engineered in a way that's fair though, will work on it.
