LCB filtering: got it, will keep the current set unless iteration speed becomes an issue.

On ensembling: that's a good idea. If concepts help some problems and hurt others, running both with and without concepts and picking the better answer could extract only the improvements. Oracle best-of would give the ceiling, then we can test whether a model-driven answer selector gets close. The pipeline already supports this — run two configs, merge at eval. Will try it.

On metacog reuse / RLAD: neither has published code. Metacog reuse is SFT-based (teacher/student training), not prompting, though their "behaviors" are one-liners for math which is close to our concepts. RLAD is RL-based but has relevant findings.

I went through all the papers in the survey more carefully. Several do eval on math/code: Dynamic Cheatsheet (DC) evals on AIME 2020-2025 and got 23% to 50% with Claude 3.5 Sonnet using a purely prompting-based self-curated memory, no training. RLAD evals on AIME 2025 and ARC-AGI, key finding is that diverse abstractions beat more solutions at large compute budgets (4 abstractions x 4 solutions > 1 abstraction x 16 solutions). LaDiR/LaDi-RL eval on AIME + LCB + MATH500 + HumanEval but are training-based (latent diffusion). Reasoning Bank evals on SWE-Bench (code) and has a MaTTS framework showing memory and test-time scaling are synergistic. Agent KB evals on SWE-bench Lite with a "disagreement gate" that checks cosine similarity between original and refined plans to prevent knowledge interference.

Most actionable for us: DC's problem-specific cheatsheet synthesis (retrieve relevant memories then synthesize a tailored hint per problem, published prompts at github.com/suzgunmirac/dynamic-cheatsheet), RLAD's finding that we should vary concept subsets across parallel attempts rather than giving the same set to all, and Reasoning Bank's approach of distilling lessons from both successful and failed trajectories.

On self-reflection: I didn't mean it can't work on math/code, more that the "default no concepts, retry with concepts on failure" pattern is less clean there. For ARC the train examples give unambiguous signal that your approach is wrong. For math/code we already have ground-truth test feedback in the retry loop, so the model already knows it failed — the question is whether adding concepts on retry specifically helps vs just retrying with the error feedback. Worth testing though, it's a simple experiment: pass 1 no concepts, pass 2 with concepts only for problems that failed pass 1.

On external reflection and LCB test cases: yes, LCB problems come with public example test cases. They're already included in the solver prompt (input/expected output pairs). The evaluator runs code against both public and private test cases. So the model already sees examples before solving. For "external reflection" in the self-correction sense: we could run the code against public tests before submitting, use the pass/fail signal to decide whether to retry with concepts. This isn't cheating since public tests are part of the problem — it's just using available information more strategically. The existing retry mechanism already feeds back test failures, but it doesn't selectively add concepts on retry, it either always has them or never does.
